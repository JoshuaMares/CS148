{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43cbc574",
   "metadata": {},
   "source": [
    "# Project 3 - Classify your own data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef714327",
   "metadata": {},
   "source": [
    "For this project we're going to explore some of the new topics since the last project including Decision Trees and Un-supervised learning. The final part of the project will ask you to perform your own data science project to classify a new dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39941a5d",
   "metadata": {},
   "source": [
    "## Submission Details\n",
    "\n",
    "**Project is due June 14th at 11:59 am (Wednesday Afternoon). To submit the project, please save the notebook\n",
    "as a pdf file and submit the assignment via Gradescope. In addition, make sure that\n",
    "all figures are legible and suﬀiciently large. For best pdf results, we recommend downloading [Latex](https://www.latex-project.org/) and print the notebook using Latex.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59b8b12",
   "metadata": {},
   "source": [
    "## Loading Essentials and Helper Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d48374c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here are a set of libraries we imported to complete this assignment. \n",
    "#Feel free to use these or equivalent libraries for your implementation\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt # this is used for the plot the graph \n",
    "import matplotlib\n",
    "import os\n",
    "import time\n",
    "#Sklearn classes\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, KFold\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix,silhouette_score\n",
    "import sklearn.metrics.cluster as smc\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder ,LabelEncoder, MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn import datasets\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "from matplotlib import pyplot\n",
    "import itertools\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "#Sets random seed\n",
    "import random \n",
    "random.seed(42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b28eaa5",
   "metadata": {
    "code_folding": [
     1,
     29,
     123,
     142,
     158
    ]
   },
   "outputs": [],
   "source": [
    "#Helper functions\n",
    "def draw_confusion_matrix(y, yhat, classes):\n",
    "    '''\n",
    "        Draws a confusion matrix for the given target and predictions\n",
    "        Adapted from scikit-learn and discussion example.\n",
    "    '''\n",
    "    plt.cla()\n",
    "    plt.clf()\n",
    "    matrix = confusion_matrix(y, yhat)\n",
    "    plt.imshow(matrix, interpolation='nearest', cmap=plt.cm.YlOrBr)\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.colorbar()\n",
    "    num_classes = len(classes)\n",
    "    plt.xticks(np.arange(num_classes), classes, rotation=0)\n",
    "    plt.yticks(np.arange(num_classes), classes)\n",
    "    plt.tick_params(top=True, bottom=False, labeltop=True, labelbottom=False)    \n",
    "    fmt = 'd'\n",
    "    thresh = matrix.max() / 2.\n",
    "    for i, j in itertools.product(range(matrix.shape[0]), range(matrix.shape[1])):\n",
    "        plt.text(j, i, format(matrix[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if matrix[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.gca().xaxis.set_label_position('top')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def heatmap(data, row_labels, col_labels, figsize = (20,12), cmap = \"YlGn\",\n",
    "            cbar_kw={}, cbarlabel=\"\", valfmt=\"{x:.2f}\",\n",
    "            textcolors=(\"black\", \"white\"), threshold=None):\n",
    "    \"\"\"\n",
    "    Create a heatmap from a numpy array and two lists of labels. \n",
    "    \n",
    "    Taken from matplotlib example.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data\n",
    "        A 2D numpy array of shape (M, N).\n",
    "    row_labels\n",
    "        A list or array of length M with the labels for the rows.\n",
    "    col_labels\n",
    "        A list or array of length N with the labels for the columns.\n",
    "    ax\n",
    "        A `matplotlib.axes.Axes` instance to which the heatmap is plotted.  If\n",
    "        not provided, use current axes or create a new one.  Optional.\n",
    "    cmap\n",
    "        A string that specifies the colormap to use. Look at matplotlib docs for information.\n",
    "        Optional.\n",
    "    cbar_kw\n",
    "        A dictionary with arguments to `matplotlib.Figure.colorbar`.  Optional.\n",
    "    cbarlabel\n",
    "        The label for the colorbar.  Optional.\n",
    "    valfmt\n",
    "        The format of the annotations inside the heatmap.  This should either\n",
    "        use the string format method, e.g. \"$ {x:.2f}\", or be a\n",
    "        `matplotlib.ticker.Formatter`.  Optional.\n",
    "    textcolors\n",
    "        A pair of colors.  The first is used for values below a threshold,\n",
    "        the second for those above.  Optional.\n",
    "    threshold\n",
    "        Value in data units according to which the colors from textcolors are\n",
    "        applied.  If None (the default) uses the middle of the colormap as\n",
    "    \"\"\"\n",
    "\n",
    "    plt.figure(figsize = figsize)\n",
    "    ax = plt.gca()\n",
    "\n",
    "    # Plot the heatmap\n",
    "    im = ax.imshow(data,cmap=cmap)\n",
    "\n",
    "    # Create colorbar\n",
    "    cbar = ax.figure.colorbar(im, ax=ax, **cbar_kw)\n",
    "    cbar.ax.set_ylabel(cbarlabel, rotation=-90, va=\"bottom\")\n",
    "\n",
    "    # Show all ticks and label them with the respective list entries.\n",
    "    ax.set_xticks(np.arange(data.shape[1]), labels=col_labels)\n",
    "    ax.set_yticks(np.arange(data.shape[0]), labels=row_labels)\n",
    "\n",
    "    # Let the horizontal axes labeling appear on top.\n",
    "    ax.tick_params(top=True, bottom=False,\n",
    "                   labeltop=True, labelbottom=False)\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=-30, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Turn spines off and create white grid.\n",
    "    ax.spines[:].set_visible(False)\n",
    "\n",
    "    ax.set_xticks(np.arange(data.shape[1]+1)-.5, minor=True)\n",
    "    ax.set_yticks(np.arange(data.shape[0]+1)-.5, minor=True)\n",
    "    ax.grid(which=\"minor\", color=\"w\", linestyle='-', linewidth=3)\n",
    "    ax.tick_params(which=\"minor\", bottom=False, left=False)\n",
    "\n",
    "    \n",
    "    # Normalize the threshold to the images color range.\n",
    "    if threshold is not None:\n",
    "        threshold = im.norm(threshold)\n",
    "    else:\n",
    "        threshold = im.norm(data.max())/2.\n",
    "\n",
    "    # Set default alignment to center, but allow it to be\n",
    "    # overwritten by textkw.\n",
    "    kw = dict(horizontalalignment=\"center\",\n",
    "              verticalalignment=\"center\")\n",
    "\n",
    "    # Get the formatter in case a string is supplied\n",
    "    if isinstance(valfmt, str):\n",
    "        valfmt = matplotlib.ticker.StrMethodFormatter(valfmt)\n",
    "\n",
    "    # Loop over the data and create a `Text` for each \"pixel\".\n",
    "    # Change the text's color depending on the data.\n",
    "    texts = []\n",
    "    for i in range(data.shape[0]):\n",
    "        for j in range(data.shape[1]):\n",
    "            kw.update(color=textcolors[int(im.norm(data[i, j]) > threshold)])\n",
    "            text = im.axes.text(j, i, valfmt(data[i, j], None), **kw)\n",
    "            texts.append(text)\n",
    "\n",
    "\n",
    "def make_meshgrid(x, y, h=0.02):\n",
    "    \"\"\"Create a mesh of points to plot in\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x: data to base x-axis meshgrid on\n",
    "    y: data to base y-axis meshgrid on\n",
    "    h: stepsize for meshgrid, optional\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    xx, yy : ndarray\n",
    "    \"\"\"\n",
    "    x_min, x_max = x.min() - 1, x.max() + 1\n",
    "    y_min, y_max = y.min() - 1, y.max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    return xx, yy\n",
    "\n",
    "\n",
    "def plot_contours(clf, xx, yy, **params):\n",
    "    \"\"\"Plot the decision boundaries for a classifier.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ax: matplotlib axes object\n",
    "    clf: a classifier\n",
    "    xx: meshgrid ndarray\n",
    "    yy: meshgrid ndarray\n",
    "    params: dictionary of params to pass to contourf, optional\n",
    "    \"\"\"\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    out = plt.contourf(xx, yy, Z, **params)\n",
    "    return out\n",
    "\n",
    "def draw_contour(x,y,clf, class_labels = [\"Negative\", \"Positive\"]):\n",
    "    \"\"\"\n",
    "    Draws a contour line for the predictor\n",
    "    \n",
    "    Assumption that x has only two features. This functions only plots the first two columns of x.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    X0, X1 = x[:, 0], x[:, 1]\n",
    "    xx0, xx1 = make_meshgrid(X0,X1)\n",
    "    \n",
    "    plt.figure(figsize = (10,6))\n",
    "    plot_contours(clf, xx0, xx1, cmap=\"PiYG\", alpha=0.8)\n",
    "    scatter=plt.scatter(X0, X1, c=y, cmap=\"PiYG\", s=30, edgecolors=\"k\")\n",
    "    plt.legend(handles=scatter.legend_elements()[0], labels=class_labels)\n",
    "\n",
    "    plt.xlim(xx0.min(), xx0.max())\n",
    "    plt.ylim(xx1.min(), xx1.max())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002d19e1",
   "metadata": {},
   "source": [
    "# Example Project using new techniques \n",
    "\n",
    "Since project 2, we have learned about a few new models for supervised learning(Decision Trees and Neural Networks) and un-supervised learning (Clustering and PCA). In this example portion, we will go over how to implement these techniques using the Sci-kit learn library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048640ad",
   "metadata": {},
   "source": [
    "## Load and Process Example Project Data\n",
    "\n",
    "\n",
    "For our example dataset, we will use the [Breast Cancer Wisconsin Dataset](https://www.kaggle.com/datasets/uciml/breast-cancer-wisconsin-data) to determine whether a mass found in a body is benign or malignant. Since this dataset was used as an example in project 2, you should be fairly familiar with it.\n",
    "\n",
    "Feature Information:\n",
    "\n",
    "Column 1: ID number\n",
    "\n",
    "Column 2: Diagnosis (M = malignant, B = benign)\n",
    "\n",
    "Ten real-valued features are computed for each cell nucleus:\n",
    "\n",
    "    1. radius (mean of distances from center to points on the perimeter)\n",
    "    2. texture (standard deviation of gray-scale values)\n",
    "    3. perimeter\n",
    "    4. area\n",
    "    5. smoothness (local variation in radius lengths)\n",
    "    6. compactness (perimeter^2 / area - 1.0)\n",
    "    7. concavity (severity of concave portions of the contour)\n",
    "    8. concave points (number of concave portions of the contour)\n",
    "    9. symmetry\n",
    "    10. fractal dimension (\"coastline approximation\" - 1)\n",
    "\n",
    "Due to the statistical nature of the test, we are not able to get exact measurements of the previous values. Instead, the dataset contains the mean and standard error of the real-valued features. \n",
    "\n",
    "Columns 3-12 present the mean of the measured values\n",
    "\n",
    "Columns 13-22 present the standard error of the measured values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817d6dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess Data\n",
    "\n",
    "#Load Data\n",
    "data = pd.read_csv('datasets/breast_cancer_data.csv')\n",
    "\n",
    "#Drop id column\n",
    "data = data.drop([\"id\"],axis= 1)\n",
    "\n",
    "#Transform target feature into numerical\n",
    "le = LabelEncoder() \n",
    "data['diagnosis'] = le.fit_transform(data['diagnosis'])\n",
    "\n",
    "#Split target and data\n",
    "y = data[\"diagnosis\"]\n",
    "x = data.drop([\"diagnosis\"],axis = 1)\n",
    "\n",
    "#Train test split\n",
    "train_raw, test_raw, target, target_test = train_test_split(x,y, test_size=0.2, stratify= y, random_state=0)\n",
    "\n",
    "#Standardize data\n",
    "#Since all features are real-valued, we only have one pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "#Transform raw data \n",
    "train = pipeline.fit_transform(train_raw)\n",
    "test = pipeline.transform(test_raw) #Note that there is no fit calls\n",
    "\n",
    "#Names of Features after Pipeline\n",
    "feature_names = list(pipeline.get_feature_names_out(list(x.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a7cffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde7902c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Baseline accuracy of using the majority class \n",
    "ct = target_test.value_counts()\n",
    "print(\"Counts of each class in target_test: \")\n",
    "print(ct)\n",
    "print(\"Baseline Accuraccy of using Majority Class: \", np.max(ct)/np.sum(ct))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257a803a",
   "metadata": {},
   "source": [
    "## Supervised Learning: Decision Tree\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53754aaa",
   "metadata": {},
   "source": [
    "### Classification with Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f98cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf = DecisionTreeClassifier(criterion=\"gini\", random_state = 0)\n",
    "clf.fit(train, target)\n",
    "predicted = clf.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5832b311",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"%-12s %f\" % ('Accuracy:', metrics.accuracy_score(target_test,predicted)))\n",
    "print(\"Confusion Matrix: \\n\", metrics.confusion_matrix(target_test,predicted))\n",
    "draw_confusion_matrix(target_test, predicted, ['healthy', 'sick'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76d80c5",
   "metadata": {},
   "source": [
    "###  Parameters for Decision Tree Classifier\n",
    "\n",
    "In Sci-kit Learn, the following are just some of the parameters we can pass into the Decision Tree Classifier:\n",
    "\n",
    "- criterion: {‘gini’, ‘entropy’, ‘log_loss’} default=\"gini\"\n",
    "    - The function to measure the quality of a split. Supported criteria are “gini” for the Gini impurity and “log_loss” and “entropy” both for the Shannon information gain \n",
    "- splitter: {“best”, “random”}, default=”best”\n",
    "    - The strategy used to choose the split at each node. “best” aims to find the best feature split amongst all features. \"random\" only looks for the best split amongst a random subset of features.\n",
    "- max_depth: int, default = 2 {‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘saga’}, default=’lbfgs’\n",
    "    - The maximum depth of the tree.\n",
    "- min_samples_split: int or float, default=2\n",
    "    - The minimum number of samples required to split an internal node. If int, then consider min_samples_split as the minimum number. If float, then min_samples_split is a fraction and ceil(min_samples_split * n_samples) are the minimum number of samples for each split."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decd1c24",
   "metadata": {},
   "source": [
    "### Visualizing Decision Trees\n",
    "\n",
    "Scikit-learn allows us to visualize the decision tree to see what features it choose to split and what the result is. Note that if the condition in the node is true, you traverse the left edge of the node. Otherwise, you traverse the right edge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc6921a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (30,15)) \n",
    "#Note that we have to pass the feature names into the plotting function to get the actual names\n",
    "#We pass the column names through the pipeline in case any feature augmentation was made\n",
    "#For example, a categorical feature will be split into multiple features with one hot encoding\n",
    "#and this way assigns a name to each column based on the feature value and the original feature name\n",
    "tree.plot_tree(clf,max_depth=1, proportion=True,feature_names=feature_names, filled=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2565ef",
   "metadata": {},
   "source": [
    "We can even look at the tree in a textual format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350c60b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_text\n",
    "r = export_text(clf, feature_names=feature_names)\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2df556",
   "metadata": {},
   "source": [
    "### Feature Importance in Decision Trees\n",
    "\n",
    "Decision Trees can also assign importance to features by measuring the average decrease in impurity (i.e. information gain) for each feature. The features with higher decreases are treated as more important. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c695f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_pd = pd.Series(data = clf.feature_importances_ ,index = feature_names)\n",
    "imp_pd= imp_pd.sort_values(ascending=False)\n",
    "imp_pd.plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef272aec",
   "metadata": {},
   "source": [
    "We can clearly see that \"concave points_mean\" has the largest importance due to it providing the most reduction in the impurity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f59dbb",
   "metadata": {},
   "source": [
    "### Visualizing decision boundaries for Decision Trees\n",
    "\n",
    "Similar to project 2, lets see what decision boundaries that a Decision Tree creates. We use the two most correlated features to the target labels: concave_points_mean and perimeter_mean. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b5b0a6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Extract first two feature and use the standardscaler \n",
    "train_2 = StandardScaler().fit_transform(train_raw[['concave points_mean','perimeter_mean']])\n",
    "\n",
    "depth  = [1,2,3,4,5,10,15]\n",
    "for d in depth:\n",
    "    dt = DecisionTreeClassifier(max_depth = d, min_samples_split=7) \n",
    "    dt.fit(train_2, target)\n",
    "    draw_contour(train_2,target,dt,class_labels = ['Benign', 'Malignant'])\n",
    "    \n",
    "    plt.title(f\"Max Depth ={d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730c6b55",
   "metadata": {},
   "source": [
    "We can see that the model gets more and more complex with increasing depth until it converges somewhere in between depth 10 and 15. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5060d5",
   "metadata": {},
   "source": [
    "## Supervised Learning: Multi-Layer Perceptron (MLP)\n",
    "\n",
    "A neural network is a series of algorithms that endeavors to recognize underlying relationships in a set of data through a process that mimics the way the human brain operates. Neural networks are very powerful tools that are used a in a variety of applications including image and speech processing. In class, we have discussed one of the earliest types of neural networks known as a Multi-Layer Perceptron. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb7df35",
   "metadata": {},
   "source": [
    "![steps](jupyter_images/mlp_example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9697d8",
   "metadata": {},
   "source": [
    "### Using MLP for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9238c9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "clf = MLPClassifier(hidden_layer_sizes=(100,), max_iter = 400)\n",
    "clf.fit(train, target)\n",
    "predicted = clf.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39a9e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"%-12s %f\" % ('Accuracy:', metrics.accuracy_score(target_test,predicted)))\n",
    "print(\"Confusion Matrix: \\n\", metrics.confusion_matrix(target_test,predicted))\n",
    "draw_confusion_matrix(target_test, predicted, ['Benign', 'Malignant'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8703bb67",
   "metadata": {},
   "source": [
    "###  Parameters for MLP Classifier\n",
    "\n",
    "In Sci-kit Learn, the following are just some of the parameters we can pass into MLP Classifier:\n",
    "\n",
    "- hidden_layer_sizes: tuple, length = n_layers - 2, default=(100,)\n",
    "    - The ith element represents the number of neurons in the ith hidden layer. \n",
    "- activation: {‘identity’, ‘logistic’, ‘tanh’, ‘relu’}, default=’relu’\n",
    "    - Activation function for the hidden layer.\n",
    "- alpha: float, default = 0.0001\n",
    "    - Strength of the L2 regularization term. The L2 regularization term is divided by the sample size when added to the loss.\n",
    "- max_iter: int, default=200\n",
    "    - Maximum number of iterations taken for the solvers to converge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae499c7",
   "metadata": {},
   "source": [
    "### Visualizing decision boundaries for MLP\n",
    "\n",
    "Now, lets see how the decision boundaries change as a function of both the activation function and the number of hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453eae3c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Example of using the default Relu activation while altering the number of hidden layers\n",
    "train_2 = StandardScaler().fit_transform(train_raw[['concave points_mean','perimeter_mean']])\n",
    "\n",
    "layers  = [50,100,150,200]\n",
    "for l in layers:\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=(l,), max_iter = 400)\n",
    "    mlp.fit(train_2, target)\n",
    "    draw_contour(train_2,target,mlp,class_labels = ['Benign', 'Malignant'])\n",
    "    \n",
    "    plt.title(f\"Hidden Layer Size ={l}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2984c343",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example of using the default Relu activation \n",
    "#while altering the number of hidden layers with 2 groups of hidden layers\n",
    "train_2 = StandardScaler().fit_transform(train_raw[['concave points_mean','perimeter_mean']])\n",
    "\n",
    "layers  = [50,100,150,200]\n",
    "for l in layers:\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=(l,l), max_iter = 400)\n",
    "    mlp.fit(train_2, target)\n",
    "    draw_contour(train_2,target,mlp,class_labels = ['Benign', 'Malignant'])\n",
    "    \n",
    "    plt.title(f\"Hidden Layer Sizes ={(l,l)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434d289f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example of using 2 hidden layers of 100 units each with varying activations\n",
    "train_2 = StandardScaler().fit_transform(train_raw[['concave points_mean','perimeter_mean']])\n",
    "\n",
    "acts  = ['identity', 'logistic', 'tanh', 'relu']\n",
    "for act in acts:\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=(100,100), activation = act, max_iter = 400)\n",
    "    mlp.fit(train_2, target)\n",
    "    draw_contour(train_2,target,mlp,class_labels = ['Benign', 'Malignant'])\n",
    "    \n",
    "    plt.title(f\"Activation = {act}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd847dd1",
   "metadata": {},
   "source": [
    "## Unsupervised learning: PCA\n",
    "\n",
    "As shown in lecture, PCA is a valuable dimensionality reduction tool that can extract a small subset of valuable features. In this section, we shall demonstrate how PCA can extract important visual features from pictures of subjects faces. We shall use the [AT&T Database of Faces](https://www.kaggle.com/datasets/kasikrit/att-database-of-faces). This dataset contains 40 different subjects with 10 samples per subject which means we a dataset of 400 samples. \n",
    "\n",
    "We extract the images from the [scikit-learn dataset library](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_olivetti_faces.html#sklearn.datasets.fetch_olivetti_faces). The library imports the images (faces.data), the flatten array of images (faces.images), and which subject eacj image belongs to (faces.target). Each image is a 64 by 64 image with pixels converted to floating point values in [0,1]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327d360b",
   "metadata": {},
   "source": [
    "### Eigenfaces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096f817f",
   "metadata": {},
   "source": [
    "The following codes downloads and loads the face data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f441df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import faces from scikit library\n",
    "faces = datasets.fetch_olivetti_faces()\n",
    "print(\"Flattened Face Data shape:\", faces.data.shape)\n",
    "print(\"Face Image Data Shape:\", faces.images.shape)\n",
    "print(\"Shape of target data:\", faces.target.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf826c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract image shape for future use\n",
    "im_shape = faces.images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95897a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prints some example faces \n",
    "faceimages = faces.images[np.random.choice(len(faces.images),size= 16, replace = False)] # take random 16 images\n",
    "\n",
    "fig, axes = plt.subplots(4,4,sharex=True,sharey=True,figsize=(8,10))\n",
    "for i in range(16):\n",
    "    axes[i%4][i//4].imshow(faceimages[i], cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130ab664",
   "metadata": {},
   "source": [
    "Now, let us see what features we can extract from these face images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d54236",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform PCA\n",
    "from sklearn.decomposition import PCA\n",
    " \n",
    "pca = PCA()\n",
    "pca_pipe = Pipeline([(\"scaler\",StandardScaler()), #Scikit learn PCA does not standardize so we need to add\n",
    "               (\"pca\",pca)])\n",
    "pca_pipe.fit(faces.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bb570b",
   "metadata": {},
   "source": [
    "The following plots the top 30 PCA components with how much variance does this feature explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50543fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 6))\n",
    "for i in range(30):\n",
    "    ax = fig.add_subplot(3, 10, i + 1, xticks=[], yticks=[])\n",
    "    ax.imshow(pca.components_[i].reshape(im_shape),\n",
    "              cmap=plt.cm.bone)\n",
    "    ax.set_title(f\"Var={pca.explained_variance_ratio_[i]:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd6a2ec",
   "metadata": {},
   "source": [
    "Amazing! We can see that the model has learned to focus on many features that we as humans also look at when trying to identify a face such as the nose,eyes, eyebrows, etc.\n",
    "\n",
    "With this feature extraction, we can perform much more powerful learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b062142",
   "metadata": {},
   "source": [
    "### Feature Extraction for Classification\n",
    "\n",
    "Lets see if we can use PCA to improve the accuracy of the decision tree classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a25ff20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Without PCA\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "clf.fit(train, target)\n",
    "predicted = clf.predict(test)\n",
    "\n",
    "print(\"Accuracy without PCA\")\n",
    "print(\"%-12s %f\" % ('Accuracy:', metrics.accuracy_score(target_test,predicted)))\n",
    "print(\"Confusion Matrix: \\n\", metrics.confusion_matrix(target_test,predicted))\n",
    "draw_confusion_matrix(target_test, predicted, ['Benign', 'Malignant'])\n",
    "\n",
    "#With PCA\n",
    "pca = PCA(n_components = 0.9) #Take components that explain at lest 90% variance\n",
    "       \n",
    "train_new = pca.fit_transform(train)\n",
    "test_new = pca.transform(test)\n",
    "\n",
    "clf_pca = DecisionTreeClassifier(random_state=0)\n",
    "clf_pca.fit(train_new, target)\n",
    "predicted = clf_pca.predict(test_new)\n",
    "\n",
    "print(\"Accuracy with PCA\")\n",
    "print(\"%-12s %f\" % ('Accuracy:', metrics.accuracy_score(target_test,predicted)))\n",
    "print(\"Confusion Matrix: \\n\", metrics.confusion_matrix(target_test,predicted))\n",
    "draw_confusion_matrix(target_test, predicted, ['Benign', 'Malignant'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910e6b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of Features without PCA: \", train.shape[1])\n",
    "print(\"Number of Features with PCA: \", train_new.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3522541a",
   "metadata": {},
   "source": [
    "Clearly, we get a much better accuracy for the model while using fewer features. But does the features the PCA thought were important the same features that the decision tree used. Lets look at the feature importance of the tree. The following plot numbers the first principal component as 0, the second as 1, and so forth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc705e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names_new = list(range(train_new.shape[1]))\n",
    "imp_pd = pd.Series(data = clf_pca.feature_importances_ ,index = feature_names_new)\n",
    "imp_pd= imp_pd.sort_values(ascending=False)\n",
    "imp_pd.plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f177147",
   "metadata": {},
   "source": [
    "Amazingly, the first and second components were the most important features in the decision tree. Thus, we can claim that PCA has significantly improved the performance of our model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afa8402",
   "metadata": {},
   "source": [
    "## Unsupervised learning: Clustering\n",
    "\n",
    "Clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense) to each other than to those in other groups. One major algorithm we learned in class is the K-Means algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1837030",
   "metadata": {},
   "source": [
    "### Evaluating K-Means performance\n",
    "\n",
    "While there are many ways to evaluate the [performance measure of clustering algorithsm](https://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation), we will focus on the inertia score of the K-Means model. Inertia is another term for the sum of squared distances of samples to their closest cluster center. \n",
    "\n",
    "Let us look at how the Inertia changes as a function of the number of clusters for an artificial dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4237ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Artifical Dataset\n",
    "X, y = make_blobs(\n",
    "    n_samples=500,\n",
    "    n_features=2,\n",
    "    centers=5,\n",
    "    cluster_std=1,\n",
    "    center_box=(-10.0, 10.0),\n",
    "    shuffle=True,\n",
    "    random_state=10,\n",
    ")  # For reproducibility\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], marker=\".\", s=30, lw=0, alpha=0.7,edgecolor=\"k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c190d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = list(range(2,10))\n",
    "inertia = []\n",
    "for k in ks:\n",
    "    kmeans = KMeans(n_clusters = k, init = 'k-means++', random_state = 0)\n",
    "    kmeans.fit(X)\n",
    "    # inertia method returns wcss for that model\n",
    "    inertia.append(kmeans.inertia_)\n",
    "    print(f\"Inertia for K = {k}: {kmeans.inertia_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75565ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(ks, inertia,marker='o',color='red')\n",
    "plt.title('The Elbow Method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f5286f",
   "metadata": {},
   "source": [
    "From the plot, we can see that when the number of clusters of K-means is the correct number of clusters, Inertia starts decreasing at a much slower rate. This creates a kind of elbow shape in the graph. For K-means clustering, the elbow method selects the number of clusters where the elbow shape is formed. In this case, we see that this method would produce the correct number of clusters.\n",
    "\n",
    "Lets try it on the cancer dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf62642",
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = list(range(2,30))\n",
    "inertia = []\n",
    "for k in ks:\n",
    "    kmeans = KMeans(n_clusters = k, init = 'k-means++', random_state = 0)\n",
    "    kmeans.fit(train)\n",
    "    # inertia method returns wcss for that model\n",
    "    inertia.append(kmeans.inertia_)\n",
    "    print(f\"Inertia for K = {k}: {kmeans.inertia_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd9ff26",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(ks, inertia,marker='o',color='red')\n",
    "plt.title('The Elbow Method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46fc860",
   "metadata": {},
   "source": [
    "Here we see that the elbow is not as cleanly defined. This may be due to the dataset not being a good fit for K-means. Regardless, we can still apply the elbow method by noting that the slow down happens around 7~14."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035024b2",
   "metadata": {},
   "source": [
    "### Kmeans on Eigenfaces\n",
    "\n",
    "Now, lets see how K-means performs in clustering the face data with PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f283e35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "n_clusters = 10 #We know there are 10 subjects\n",
    "km = KMeans(n_clusters  = n_clusters,random_state=0)\n",
    "\n",
    "pipe= Pipeline([(\"scaler\",StandardScaler()), #First standardize\n",
    "               (\"pca\",PCA()), #Transform using pca\n",
    "         (\"kmeans\", km )]) #Then apply k means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2937f475",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = pipe.fit_predict(faces.data)\n",
    "print(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6f4ebf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for labelID in range(n_clusters):\n",
    "    # find all indexes into the `data` array that belong to the\n",
    "    # current label ID, then randomly sample a maximum of 25 indexes\n",
    "    # from the set\n",
    "    idxs = np.where(clusters == labelID)[0]\n",
    "    idxs = np.random.choice(idxs, size=min(25, len(idxs)),\n",
    "        replace=False)\n",
    "\n",
    "    # Extract the sampled indexes\n",
    "    id_face = faces.images[idxs]\n",
    "\n",
    "    #Plots sampled faces\n",
    "    fig = plt.figure(figsize=(10,5))\n",
    "    for i in range(min(25,len(idxs))):\n",
    "        ax = fig.add_subplot(5, 5, i + 1, xticks=[], yticks=[])\n",
    "        ax.imshow(id_face[i],\n",
    "                  cmap=plt.cm.bone)\n",
    "    fig.suptitle(f\"Id={labelID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db5d278",
   "metadata": {},
   "source": [
    "While the algorithm isn't perfect, we can see that K-means with PCA is picking up on some facial similarity or similar expressions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c1f820",
   "metadata": {},
   "source": [
    "# (100 pts) Todo: Use new methods to classify heart disease\n",
    "\n",
    "To compare how these new models perform with the other models discussed in the course, we will apply these new models on the heart disease dataset that was used in project 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8a23ff",
   "metadata": {},
   "source": [
    "## Background: The Dataset (Recap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2537241",
   "metadata": {},
   "source": [
    "For this exercise we will be using a subset of the UCI Heart Disease dataset, leveraging the fourteen most commonly used attributes. All identifying information about the patient has been scrubbed. You will be asked to classify whether a <b>patient is suffering from heart disease</b> based on a host of potential medical factors.\n",
    "\n",
    "The dataset includes 14 columns. The information provided by each column is as follows:\n",
    "<ul>\n",
    "    <li><b>age:</b> Age in years</li>\n",
    "    <li><b>sex:</b> (1 = male; 0 = female)</li>\n",
    "    <li><b>cp:</b> Chest pain type (0 = asymptomatic; 1 = atypical angina; 2 = non-anginal pain; 3 = typical angina)</li>\n",
    "    <li><b>trestbps:</b> Resting blood pressure (in mm Hg on admission to the hospital)</li>\n",
    "    <li><b>chol:</b> cholesterol in mg/dl</li>\n",
    "    <li><b>fbs</b> Fasting blood sugar > 120 mg/dl (1 = true; 0 = false)</li>\n",
    "    <li><b>restecg:</b> Resting electrocardiographic results (0= showing probable or definite left ventricular hypertrophy by Estes' criteria; 1 = normal; 2 = having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV))</li>\n",
    "    <li><b>thalach:</b> Maximum heart rate achieved</li>\n",
    "    <li><b>exang:</b> Exercise induced angina (1 = yes; 0 = no)</li>\n",
    "    <li><b>oldpeak:</b> Depression induced by exercise relative to rest</li>\n",
    "    <li><b>slope:</b> The slope of the peak exercise ST segment (0 = downsloping; 1 = flat; 2 = upsloping)</li>\n",
    "    <li><b>ca:</b> Number of major vessels (0-3) colored by flourosopy</li>\n",
    "    <li><b>thal:</b> 1 = normal; 2 = fixed defect; 7 = reversable defect</li>\n",
    "    <li><b><u>sick:</u></b> Indicates the presence of Heart disease (True = Disease; False = No disease)</li>\n",
    "</ul>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0222d80e",
   "metadata": {},
   "source": [
    "## Preprocess Data\n",
    "\n",
    "This part is done for you since you would have already completed it in project 2. Use the train, target, test, and target_test for all future parts. We also provide the column names for each transformed column for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecf5078",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess Data\n",
    "\n",
    "#Load Data\n",
    "data = pd.read_csv('datasets/heartdisease.csv')\n",
    "\n",
    "#Transform target feature into numerical\n",
    "le = LabelEncoder() \n",
    "data['target'] = le.fit_transform(data['sick'])\n",
    "data = data.drop([\"sick\"], axis =1)\n",
    "\n",
    "#Split target and data\n",
    "y = data[\"target\"]\n",
    "x = data.drop([\"target\"],axis = 1)\n",
    "\n",
    "#Train test split\n",
    "#40% in test data as was in project 2\n",
    "train_raw, test_raw, target, target_test = train_test_split(x,y, test_size=0.4, stratify= y, random_state=0)\n",
    "\n",
    "#Feature Transformation\n",
    "#This is the only change from project 2 since we replaced standard scaler to minmax\n",
    "#This was done to ensure that the numerical features were still of the same scale\n",
    "#as the one hot encoded features\n",
    "num_pipeline = Pipeline([\n",
    "    ('minmax', MinMaxScaler()) \n",
    "])\n",
    "\n",
    "heart_num = train_raw.drop(['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca','thal'], axis=1)\n",
    "numerical_features = list(heart_num)\n",
    "categorical_features = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca','thal']\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "    (\"num\", num_pipeline, numerical_features),\n",
    "    (\"cat\", OneHotEncoder(categories='auto'), categorical_features),\n",
    "])\n",
    "\n",
    "#Transform raw data \n",
    "train = full_pipeline.fit_transform(train_raw)\n",
    "test = full_pipeline.transform(test_raw) #Note that there is no fit calls\n",
    "\n",
    "#Extracts features names for each transformed column\n",
    "feature_names = full_pipeline.get_feature_names_out(list(x.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3981e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Column names after transformation by pipeline: \", feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358543d6",
   "metadata": {},
   "source": [
    "The following shows the baseline accuracy of simply classifying every sample as the majority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bba8f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Baseline accuracy of using the majority class \n",
    "ct = target_test.value_counts()\n",
    "print(\"Counts of each class in target_test: \")\n",
    "print(ct)\n",
    "print(\"Baseline Accuraccy of using Majority Class: \", np.max(ct)/np.sum(ct))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdc7c36",
   "metadata": {},
   "source": [
    "## (25 pts) Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cef6b6",
   "metadata": {},
   "source": [
    "### [5 pts] Apply  Decision Tree on Train Data\n",
    "\n",
    "Apply the decision tree on the **train data** with default parameters of the DecisionTreeClassifier. **Report the accuracy and print the confusion matrix**. Make sure to use random_state = 0 so that your results match ours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebf223f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef9502c0",
   "metadata": {},
   "source": [
    "### [5 pts] Visualize the Decision Tree\n",
    "\n",
    "Visualize the first two layers of the decision tree that you trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafbd64b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "662f49b2",
   "metadata": {},
   "source": [
    "**What is the gini index improvement of the first split?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2218a1",
   "metadata": {},
   "source": [
    "Response:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e871b8ec",
   "metadata": {},
   "source": [
    "### [5 pts] Plot the importance of each feature for the Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122cb3fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "001cc654",
   "metadata": {},
   "source": [
    "**How many features have non-zero importance for the Decision Tree? If we remove the features with zero importance, will it change the decision tree for the same sampled dataset?** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef03a367",
   "metadata": {},
   "source": [
    "Response: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e9c946",
   "metadata": {},
   "source": [
    "### [10 pts] Optimize Decision Tree\n",
    "\n",
    "While the default Decision Tree performs fairly well on the data, lets see if we can improve performance by optimizing the parameters.\n",
    "\n",
    "Run a GridSearchCV with 3-Fold Cross Validation for the Decision Tree. Find the best model parameters amongst the following:\n",
    "\n",
    "- max_depth = [1,2,3,4,5,10,15]\n",
    "- min_samples_split = [2,4,6,8]\n",
    "- criterion = [\"gini\", \"entropy\"]\n",
    "\n",
    "After using GridSearchCV, print the best model parameters and the best score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a61e750",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "922d6780",
   "metadata": {},
   "source": [
    "**Using the best model you have, report the test accuracy and print out the confusion matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871ba08f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e5ca8804",
   "metadata": {},
   "source": [
    "## (20 pts) Multi-Layer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fff664d",
   "metadata": {},
   "source": [
    "### [5 pts] Applying a Multi-Layer Perceptron\n",
    "Apply the MLP on the **train data** with hidden_layer_sizes=(100,100) and max_iter = 800. **Report the accuracy and print the confusion matrix**. Make sure to set random_state=0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94307d8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d0971cfc",
   "metadata": {},
   "source": [
    "### [10 pts] Speedtest between Decision Tree and MLP\n",
    "\n",
    "Let us compare the training times and prediction times of a Decision Tree and an MLP. **Time how long it takes for a Decision Tree and an MLP to perform a .fit operation (i.e. training the model). Then, time how long it takes for a Decision Tree and an MLP to perform a .predict operation (i.e. predicting the testing data). Print out the timings and specify which model was quicker for each operation.** We recommend using the [time](https://docs.python.org/3/library/time.html) python module to time your code. \n",
    "An example of the time module was shown in project 2. Use the default Decision Tree Classifier and the MLP with the previously mentioned parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139184bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "59d1dbef",
   "metadata": {},
   "source": [
    "### [5 pts] Compare and contrast Decision Trees and MLPs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3754e80b",
   "metadata": {},
   "source": [
    "**Describe at least one advantage and disadvantage of using an MLP over a Decision Tree.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cd11f7",
   "metadata": {},
   "source": [
    "Response:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73a10fe",
   "metadata": {},
   "source": [
    "## (35 pts) PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdfb6f5",
   "metadata": {},
   "source": [
    "### [5 pts] Transform the train data using PCA\n",
    "\n",
    "Train a PCA model to project the train data on the top 10 components. **Print out the 10 principal components**. Look at the documentation of [PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f11df5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0af4c883",
   "metadata": {},
   "source": [
    "### [5 pts] Percentage of variance explained by top 10 principal components\n",
    "\n",
    "Using PCA's \"explained_variance_ratio_\", print the percentage of variance explained by the top 10 principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3744e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0d6410e",
   "metadata": {},
   "source": [
    "### [5 pts] Transform the train and test data into train_pca and test_pca using PCA\n",
    "\n",
    "Note: Use fit_transform for train and transform for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e8d558",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0cc561b5",
   "metadata": {},
   "source": [
    "### [5 pts] PCA+Decision Tree\n",
    "\n",
    "Train the default Decision Tree Classifier using train_pca. **Report the accuracy using test_pca and print the confusion matrix**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f424280",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e2e1af68",
   "metadata": {},
   "source": [
    "**Does the model perform better with or without PCA?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ead722a",
   "metadata": {},
   "source": [
    "Response:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6a82a9",
   "metadata": {},
   "source": [
    "### [5 pts] PCA+MLP\n",
    "\n",
    "Train the MLP classifier with the same parameters as before using train_pca. **Report the accuracy using test_pca and print the confusion matrix**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc7659b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f2b4675",
   "metadata": {},
   "source": [
    "**Does the model perform better with or without PCA?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c440eaad",
   "metadata": {},
   "source": [
    "Response:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a45ada",
   "metadata": {},
   "source": [
    "### [10 pts] Pros and Cons of PCA\n",
    "\n",
    "**In your own words, provide at least two pros and at least two cons for using PCA**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2e6150",
   "metadata": {},
   "source": [
    "Response:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095b4f9f",
   "metadata": {},
   "source": [
    "## (20 pts) K-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc67ded9",
   "metadata": {},
   "source": [
    "### [5 pts] Apply K-means to the train data and print out the Inertia score\n",
    "\n",
    "Use n_cluster = 5 and random_state = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0de801",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4c272972",
   "metadata": {},
   "source": [
    "### [10 pts] Find the optimal cluster size using the elbow method. \n",
    "\n",
    "Use the elbow method to find the best cluster size or range of best cluster sizes for the train data. Check the cluster sizes from 2 to 20. Make sure to plot the Inertia and state where you think the elbow starts. Make sure to use random_state = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc7659f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "350e9a88",
   "metadata": {},
   "source": [
    "### [5 pts] Find the optimal cluster size for the train_pca data\n",
    " \n",
    "Repeat the same experiment but use train_pca instead of train. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697a5842",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6a2dc068",
   "metadata": {},
   "source": [
    "**Notice that the inertia is much smaller for every cluster size when using PCA features. Why do you think this is happening? Hint: Think about what Inertia is calculating and consider the number of features that PCA outputs.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2213c5",
   "metadata": {},
   "source": [
    "Response:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b27da2",
   "metadata": {},
   "source": [
    "# (100 pts) Putting it all together\n",
    "\n",
    "Through all the homeworks and projects, you have learned how to apply many different models to perform a supervised learning task. We are now asking you to take everything that you learned to create a model that can predict whether a hotel reservation will be canceled or not.\n",
    "\n",
    "\n",
    "**Context**\n",
    "\n",
    "Hotels see millions of people every year and always wants to keep rooms occupied and payed for. Cancellations make the business lose money since it may make it difficult to reserve to another customer on such short notice. As such, it is useful for a hotel to know whether a reservation is likely to cancel or not. The following dataset will provide a variety of information about a booking that you will use to predict whether that booking will cancel or not. \n",
    "\n",
    "\n",
    "Property Management System - PMS\n",
    "\n",
    "**Attribute Information**\n",
    "\n",
    "(C) is for Categorical \n",
    "\n",
    "(N) is for Numeric\n",
    "\n",
    "    1) is_canceled (C) : Value indicating if the booking was canceled (1) or not (0).\n",
    "    2) hotel (C) : The datasets contains the booking information of two hotel. One of the hotels is a resort hotel and the other is a city hotel.\n",
    "    3) arrival_date_month (C): Month of arrival date with 12 categories: “January” to “December”\n",
    "    4) stays_in_weekend_nights (N): Number of weekend nights (Saturday or Sunday) the guest stayed or booked to stay at the hotel\n",
    "    5) stays_in_week_nights (N): Number of week nights (Monday to Friday) the guest stayed or booked to stay at the hotel BO and BL/Calculated by counting the number of week nights\n",
    "    6) adults (N): Number of adults\n",
    "    7) children (N): Number of children\n",
    "    8) babies (N): Number of babies\n",
    "    9) meal (C): Type of meal\n",
    "    10) country (C): Country of origin.\n",
    "    11) previous_cancellations (N): Number of previous bookings that were canceled by the customer prior to the current booking\n",
    "    12) previous_bookings_not_canceled (N) : Number of previous bookings not canceled by the customer prior to the current booking\n",
    "    13) reserved_room_type (C): Code of room type reserved. Code is presented instead of designation for anonymity reasons\n",
    "    14) booking_changes (N) : Number of changes/amendments made to the booking from the moment the booking was entered on the PMS until the moment of check-in or cancellation\n",
    "    15) deposit_type (C) : No Deposit – no deposit was made; Non Refund – a deposit was made in the value of the total stay cost; Refundable – a deposit was made with a value under the total cost of stay\n",
    "    16) days_in_waiting_list (N): Number of days the booking was in the waiting list before it was confirmed to the customer\n",
    "    17) customer_type (C): Group – when the booking is associated to a group; Transient – when the booking is not part of a group or contract, and is not associated to other transient booking; Transient-party – when the booking is transient, but is associated to at least other transient booking\n",
    "    18) adr (N): Average Daily Rate (Calculated by dividing the sum of all lodging transactions by the total number of staying nights)\n",
    "    19) required_car_parking_spaces (N): Number of car parking spaces required by the customer\n",
    "    20) total_of_special_requests (N): Number of special requests made by the customer (e.g. twin bed or high floor)\n",
    "    21) name (C): Name of the Guest (Not Real)\n",
    "    22) email (C): Email (Not Real)\n",
    "    23) phone-number (C): Phone number (not real)\n",
    "\n",
    "This dataset is quite large with 86989 samples. This makes it difficult to just brute force running a lot of models. As such, you have to be thoughtful when designing your models.\n",
    "\n",
    "The file name for the training data is \"hotel_booking.csv\".\n",
    "\n",
    "**Challenge**\n",
    "\n",
    "This project is about being able to predict whether a reservation is likely to cancel based on the\n",
    "input parameters available to us. We will ask you to perform some specific instructions to lead you in the right direction but you are given free reign on which models to use and the preprocessing steps you make. We will ask you to **write out a description of what models you choose and why you choose them**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fa9cfc",
   "metadata": {},
   "source": [
    "## (50 pts) Preprocessing\n",
    "\n",
    "\n",
    "**Preprocessing:**\n",
    "For the dataset, the following are mandatory pre-processing steps for your data:\n",
    "\n",
    "- Use One-Hot Encoding on all categorical features (specify whether you keep the extra feature or not for features with multiple values)\n",
    "- Determine which fields need to be dropped\n",
    "- Handle missing values (Specify your strategy)\n",
    "- Rescale the real valued features using any strategy you choose (StandardScaler, MinMaxScaler, Normalizer, etc)\n",
    "- Augment at least one feature\n",
    "- Implement a train-test split with 20% of the data going to the test data. Make sure that the test and train data are balanced in terms of the desired class.\n",
    "\n",
    "\n",
    "After writing your preprocessing code, write out a description of what you did for each step and provide a justification for your choices. All descriptions should be written in the markdown cells of the jupyter notebook. Make sure your writing is clear and professional.  \n",
    "\n",
    "We highly recommend reading through the [scikit-learn documentation](https://scikit-learn.org/stable/data_transforms.html) to make this part easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f524ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c1b9a682",
   "metadata": {},
   "source": [
    "## (50 pts) Try out a few models\n",
    "Now that you have pre-processed your data, you are ready to try out different models. \n",
    "\n",
    "For this part of the project, we want you to experiment with all the different models demonstrated in the course to determine which one performs best on the dataset.\n",
    "\n",
    "You must perform classification using at least 3 of the following models:\n",
    "- Logistic Regression\n",
    "- K-nearest neighbors\n",
    "- SVM\n",
    "- Decision Tree\n",
    "- Multi-Layer Perceptron\n",
    "\n",
    "Due to the size of the dataset, be careful which models you use and look at their documentation to see how you should tackle this size issue for each model.\n",
    "\n",
    "For full credit, you must perform some hyperparameter optimization on your models of choice. You may find the following scikit-learn library on [hyperparameter optimization](https://scikit-learn.org/stable/modules/grid_search.html#grid-search) useful.\n",
    "\n",
    "For each model chosen, write a description of which models were chosen, which parameters you optimized, and which parameters you choose for your best model. \n",
    "While the previous part of the project asked you to pre-process the data in a specific manner, you may alter pre-processing step as you wish to adjust for your chosen classification models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a08c263",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4bb7d356",
   "metadata": {},
   "source": [
    "## Extra Credit \n",
    "\n",
    "We have provided an extra test dataset named \"hotel_booking_test.csv\" that does not have the target labels. Classify the samples in the dataset with your best model and write them into a csv file. Submit your csv file to our [Kaggle](https://www.kaggle.com/t/ed725123bf124e9199c1d8fdc8a2d9c7) contest. The website will specify your classification accuracy on the test set. We will award a bonus point for the project for every percentage point over 75% that you get on your kaggle test accuracy.\n",
    "\n",
    "To get the bonus points, you must also write out a summary of the model that you submit including any changes you made to the pre-processing steps. The summary must be written in a markdown cell of the jupyter notebook. Note that you should not change earlier parts of the project to complete the extra credit. \n",
    "\n",
    "**Kaggle Submission Instruction**\n",
    "Submit a two column csv where the first column is named \"ID\" and is the row number. The second column is named \"target\" and is the classification for each sample. Make sure that the sample order is preserved."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "429px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
